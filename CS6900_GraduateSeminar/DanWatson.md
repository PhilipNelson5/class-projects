---
title: 'Dr. Dan Watson'
author: Philip Nelson
date: 25th September 2019
---

Dr. Watson's major research areas are algorithms for massive parallel processors, parallel systems.

Two researchers in the filed of parallel computing are Dr. Thomas Fahringer at the University of Innsbruck and Dr. Angeles G. Navarro at the University of Malaga.

Dr. Fahringer published a paper entitled "Exascale Machines Require New Programming Paradigms and Runtimes" [https://superfri.org/superfri/article/view/44](https://superfri.org/superfri/article/view/44). Dr. Fahringer starts by describing exascale computing systems as extreme scale parallel computing systems with tens of thousands of nodes with hundreds of cores each as well as deep memory hierarchies and complex interconnection topologies. These exascale systems provide huge hardware parallelism but also introduce higher failure rates. Exascale systems programming already requires huge efforts to achieve high efficiency. Unfortunately these programs often end up being platform specific thus non-portable. Dr. Fahringer then discusses the shortcomings of these current methods. The first model discussed is the message passing programming model, then the shared-memory programming model, and the heterogeneous programming model. Dr. Fahringer then discusses a promising hybrid model he describes as MPI+X, where X is a programming model that supports threads. Often a common choice is OpenMP. This hybrid model supports message passing for inter process communication and shared memory for intra process communication. Using a model as described could get programs near exascale.

Dr. Fahringer published a paper entitled "Performance Analysis of Cloud Computing Services for Many-Tasks Scientific Computing" [https://ieeexplore.ieee.org/document/5719609](https://ieeexplore.ieee.org/document/5719609). In this paper, Dr. Fahringer discusses cloud computing as a solution to individual companies maintaining expensive computing facilities. Through virtualization and resource sharing, cloud computing systems can share their extensive resources with multiple users with diverse needs. However, at the time, current commercially available clouds have been designed with web and small database workloads in mind. These workloads are very different from typical scientific computing workloads. To make matters worse, virtualization and resource time sharing may incur significant performance penalties for the demanding scientific computing workloads. Dr. Fahringer then discusses the performance of cloud computing services for scientific computing workloads. The problem areas that Dr. Fahringer identifies are job structure, bottleneck resources, and job parallelism. Dr. Fahringer and his group performed performance evaluations of four public computing clouds, including Amazon EC2, one of the largest commercial clouds currently in production. The main finding was that the performance of the tested clouds is low. They plan to extendd the work with more analysis of other cloud services, in particular storage and network services.

Dr. Malaga published a paper entitled "Simultaneous multiprocessing in a software-defined
heterogeneous FPGA" [https://link.springer.com/content/pdf/10.1007/s11227-018-2367-9.pdf](https://link.springer.com/content/pdf/10.1007/s11227-018-2367-9.pdf). Dr Malaga discusses heterogeneous compute systems as a path forward to deliver energy and performance requirements that we demand for the future. A heterogeneous architecture uses specialized hardware units to accelerate complex tasks. An example of a heterogeneous architecture is using a graphical processing unit (GPU) along side a central processing unit (CPU). The GPU takes the task of processing the graphics while the CPU is left for other parts of the program. Field programmable gate arrays (FPGA) are an alternative technology. They offer bit level parallelism in contrast to word level parallelism in CPUs and GPUs. This finer grained parallelism is well suited for some problems and algorithms that are not easily solvable with world level parallelism systems. One of the barriers to entry for FPGA design is the need to know low level programming languages. Luckily, this is starting to be successfully replaced by higher level languages such as C++ and OpenCL. In her research, Dr. Malaga looks at changing the relationship between the CPU and FPGA in a heterogeneous system. Instead of offloading the work to the FPGA and then idling until the work is done, her system used a scheduler to allow the FPGA and CPU to work in parallel. For a compute intensive application, her team was able to obtain up to 45% more throughput and 18% less energy consumption when all devices of a Zynq-7000 SoC collaborate in the computation compared against FPGA-only execution.

Dr. Malaga published a paper entitled "Strategies for maximizing utilization on multi-CPU
and multi-GPU heterogeneous architectures" [https://link.springer.com/content/pdf/10.1007/s11227-014-1200-3.pdf](https://link.springer.com/content/pdf/10.1007/s11227-014-1200-3.pdf). In this paper, Dr Malaga explores the possibility of executing a single application using multiple GPUs in a parallel task programming paradigm. The paper particularly addresses the parallel for loop to allow it to take advantage of all the resources of a heterogeneously architected system. They present a dynamic scheduling system with an adaptive partitioning scheme that resize chunks in order to prevent load imbalances between the CPUs and GPUs. Their findings were that a collaborative host thread strategy implemented at the application level can be outperformed by a non collaborative host thread strategy combined with a blocking synchronization mechanism, when moderate oversubscription controlled by the OS is allowed.

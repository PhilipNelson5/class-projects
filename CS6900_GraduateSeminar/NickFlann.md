---
title: 'Dr. Nick Flann'
author: Philip Nelson
date: 16th October 2019
header-includes: |
    \usepackage{ amssymb }
---

Dr. Flann's major research areas are deep learning, algorithmic differentiation, computational systems biology, complexity of living systems, and multiscale modeling of biological systems.

Dr. Flann's research group has been working on

* Modeling cell differentiation
* Other physical and biological processes
* Using machine learning to support and solve environmental problems

I liked the cell differentiation project because it was fascinating to see how algorithmic differentiation is able to simplify the modeling of complex biological processes. That was the focus of Dr. Flann's presentation. 

Two researchers in the filed of algorithmic differentiation and machine learning are Dr. Sebastian F. Walter at Institut für Mathematik, Fakultät Math. Nat. II, Humboldt-Universität zu Berlin and Dr. Alán Aspuru-Guzik at University of Toronto.

Dr. Walter published a paper entitled "Algorithmic differentiation in Python with AlgoPy" [https://www.sciencedirect.com/science/article/pii/S1877750311001013](https://www.sciencedirect.com/science/article/pii/S1877750311001013). In this paper, Dr. Walter addresses how a python called "AlgoPy" is filling a need in scientific computing. Often in scientific computing, the module "NumPy" is used to take care of heavy numeric linear algebra (NLA) functions. It is also used in vectorized operations, slicing and broadcasting. AlgoPy on the otherhand provides the means to compute arbitrary order derivatives and Taylor approximations of programs. AlgoPy is based on the combination of univariate Taylor polynomial arithmetic and matrix calculus in the combined forward/reverse mode of Algorithmic Differentiation. The key difference that AlgoPy provides, in contrast to existing AD tools, is that vectorized operations and NLA functions are not considered to be a sequence of scalar elementary functions. Instead, dedicated algorithms for the matrix product, matrix inverse and the Cholesky, QR, and symmetric eigenvalue decomposition are implemented in AlgoPy.

To evaluate AlgoPy, Dr. Walter et al. benchmarked a minimal surface problem, and ODE fitting. The runtime comparison revealed that the current implementation of AlgoPy suffers speed-wise from a relatively large overhead. A reduction of this overhead could be a next step Walter notes. He believes that code generation techniques similar to TheanoIt would also be a good idea to make all algorithms truly generic. With such a feature it would be easy to compute nested derivatives of the form $\triangledown g(\triangledown f(x))$.

Dr. Walter published a paper entitled "On evaluating higher-order derivatives of the QR decomposition of tall matrices with full column rank in forward and reverse mode algorithmic differentiation" [https://www.mendeley.com/catalogue/evaluating-higherorder-derivatives-qr-decomposition-tall-matrices-full-column-rank-forward-reverse-m/](https://www.mendeley.com/catalogue/evaluating-higherorder-derivatives-qr-decomposition-tall-matrices-full-column-rank-forward-reverse-m/). In this paper, Dr. Walter addresses the task of higher-order derivative evaluation of computer programs that contain QR decompositions of tall matrices with full column rank. The approach Dr. Walter takes is a combination of univariate Taylor polynomial arithmetic and matrix calculus in the (combined) forward/reverse mode of algorithmic differentiation (AD). Explicit algorithms are derived by Walter et al.

The reason to investigate this topic is due to the fact that derivatives and polynomial approximations of functions $F:\mathbb{R}^n \rightarrow \mathbb{R}^m$ in multiple variables play a central role in the solution of scientific and engineering problems. In scientific computing such functions are described by algorithms. The basic idea of algorithmic differentiation (AD) is to break down the overall problem of evaluating derivatives into smaller subtasks by the application of the chain rule and generalizations thereof. This reduction allows to evaluate first- and higher-order derivatives of even very complex computational models as long as each subtask is differentiable.

Dr. Aspuru-Guzik published a paper entitled "Accelerating the discovery of materials for clean energy in the era of smart automation" [https://www.nature.com/articles/s41578-018-0005-z](https://www.nature.com/articles/s41578-018-0005-z). In this paper, Dr. Aspuru-Guzik discusses the discovery and development of new and novel materials in the field of clean energy. He asserts that these new materials are essential to accelerate the transition to a low-carbon economy. With the recent technological innovations in automation, computer science, and robotics in conjunction with current understanding of chemistry, material synthesis and characterization, will be an accelerant of in revolutionizing traditional research and development in both industry and academia.

Dr. Aspuru-Guzik's perspective provides a vision for an integrated artificial intelligence approach towards autonomous materials discovery. In their opinion, this will emerge within the next five to ten years. The approach discussed in the paper requires the integration of the tools, which have already seen substantial development to date: high-throughput virtual screening, automated synthesis planning, automated laboratories and machine learning algorithms. In conjunction with reducing the time to deployment of new materials, this integrated approach is expected by Dr. Aspuru-Guzik to lower the cost associated with the initial discovery. This way, the price of the final products (solar panels, batteries, electric vehicles, etc) will also decrease. This in turn will enable industries and governments to meet more ambitious targets in terms of reducing greenhouse gas emissions at a faster pace.


Dr. Aspuru-Guzik published a paper entitled "Learning from the Harvard Clean Energy Project: The Use of Neural Networks to Accelerate Materials Discovery" [https://onlinelibrary.wiley.com/doi/full/10.1002/adfm.201501919](https://onlinelibrary.wiley.com/doi/full/10.1002/adfm.201501919). In this paper, Dr. Aspuru-Guzik discusses the employment of multilayer perceptrons, a type of artificial neural network. It is proposed as part of a computational funneling procedure for high‐throughput organic materials design. Through the use of state of the art algorithms and a large amount of data extracted from the Harvard Clean Energy Project, it is demonstrated that these methods allow a great reduction in the fraction of the screening library that is actually calculated. Neural networks can reproduce the results of quantum‐chemical calculations with a high degree of accuracy. The approach proposed by Dr. Aspuru-Guzik allows execution out of large‐scale molecular screening projects with less computational time. This then allows for the exploration of increasingly large and diverse libraries.

The most common bottleneck of molecular materials design is the experimental synthesis and characterization of a material. The problem of selecting the most promising material to explore experimentally is at the forefront of materials discovery. Theoretical techniques have shown some significant successes in narrowing a potential list of candidate molecules. In the Harvard Clean Energy Project (CEP), a exploration of a library of just 26 fragments using only two basic molecular combination rules resulted in the calculation of properties for over 3.5 million molecules and required a specialized distributed computing framework to compute the 30,000 CPU years expended. Since chemical space—even that localized to some specific desired property—is many orders of magnitude larger than the 26 fragments studied in the Clean Energy Project, and can be combined in countless more ways, a way to search greater areas of chemical space without the resultant explosion of computational expense is required for high throughput virtual screening to fulfill its great potential.

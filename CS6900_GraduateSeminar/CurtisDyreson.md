---
title: 'Dr. Curtis Dyreson'
author: Philip Nelson
date: 18th September 2019
---

Dr. Curtis Dyreson's major research area is in databases.

Dr. Curtis Dyreson's research group has been working on the following project:

* Using CouchDB to Compute Temporal Aggregation
  * Using map reduce to perform the query
  * Leverage CouchDB's B-Tree query representation to count the items in the aggregation

I found Dr. Dyreson's research very interesting because it showed a new approach to solving a problem in a clever way. I thought it was clever how he was able to leverage CouchDB's B-Tree query representation to count the items in the temporal aggregation. It was also interesting to listen to his presentation because I am currently taking his course on databases. Since he only spoke about one project, there were no projects I found unappealing.

Two researchers in the filed of Databases are Dr. Stanley Zdonik at Brown University and Dr. Mike Cafarella at University of Michigan.

Dr. Zdonik published a paper entitled "Larger-than-memory data management on modern storage hardware for in-memory OLTP database systems" [https://dl.acm.org/citation.cfm?id=2933358](https://dl.acm.org/citation.cfm?id=2933358). In this paper, Dr Zdonik discusses the advantages and shortcomings of in-memory database management systems. He then discusses how to overcome the limitations. In-memory database management systems are much faster compared to disk-oriented ones however they can only manage a database that is small enough to be held in memory. This problem has been known for a long time and effect modern database management systems such as MongoDB. The current method for allowing larger than memory databases to be managed by in-memory database management systems is similar to how an operating system manages memory through paging. The failing point for the database management systems is that it does not know if a page is in memory thus it can not know when it will have a page-fault so transactions are stalled while the page is loaded into memory. They analyzed methods both independent and tightly coupled to the underlying storage technologies including HDD, SSD, 3DX, and NVRAM. They then evaluated these methods in the H-Store DBMS. Their results showed that tailoring the strategy for each storage technology improves throughput by up to three times over a generic configuration. They also found that smaller block sizes and synchronous retrieval policy are generally good choices for storage devices that have low access latencies, such as SSD, 3DX, and NVRAM. 

Dr. Zdonik published a paper entitled "Bridging the Archipelago between Row-Stores and Column-Stores for Hybrid Workloads" [https://dl.acm.org/citation.cfm?id=2915231](https://dl.acm.org/citation.cfm?id=2915231). In this paper, Dr. Zdonik explores a solution to modern data-intensive applications. The problem Dr. Zdonik has identified in modern data-intensive applications is that they often desire to analyze a combination of historical data sets alongside recently collected data. In order to support these hybrid workloads, database management systems need to handle fast ACID transactions and complex analytical queries on the same database. The current solution is to use independent specialized systems that are optimized for one of these workloads, and thus require separate copies of the database. This adds unnecessary cost to deploying a database application both in terms of storage and administrative overhead. To overcome this limitation, Dr. Zdonik presents a hybrid database management system architecture that supports varied workloads on the same database. Their solution is different from current ones because they implement a single execution engine that is abstracted from to the storage layout of data. They were able to do so without sacrificing performance benefits exhibited in the specialized systems. This joint solution removes the need to maintain two separate copies of the database in independent database management systems. Dr. Zdonik's results showed that their approach delivered up to three times higher throughput compared to current systems.

Dr. Cafarella published a paper entitled "A declarative query processing system for nowcasting" [https://dl.acm.org/citation.cfm?id=3021931](https://dl.acm.org/citation.cfm?id=3021931). In this paper, Dr. Cafarella describes nowcasting, which is the practice of using social media data to quantify ongoing real-world phenomena. nowcasting has been used to measure such trends as flu activity and unemployment. However, typical nowcasting work flows require slow and tedious manual searching of relevant social media messages or automated statistical approaches which are prone to spurious and low-quality results. In the paper, Dr. Cafarella proposes a method for declaratively specifying a nowcasting model. His method involves processing a user's query over a large social media database, an operation which can take hours to execute. Since the method requires a human-in-the-loop to construct the nowcasting models, slow runtimes put a large burden on the user. Dr Cafarella also proposes a novel set of query optimization techniques, which allow users to quickly construct nowcasting models over very large datasets. Further, he proposes a novel query quality alarm that helps users estimate phenomena even when historical ground truth data is not available. These new methods allow his team to build a declarative nowcasting database management system called RaccoonDB. RaccoonDB yields high-quality results in interactive time. They evaluated RaccoonDB using 40 billion tweets. They showed that their automated system was less work for analysts to use compared to traditional manual approaches and improved result quality, producing 57% more accurate results in their user study, and that RaccoonDB's query optimizations yielded a 424 time speedup, allowing it to process queries 123 times faster than a 300-core Spark cluster, using only 10% of the computational resources.

Dr. Cafarella published a paper entitled "Structured Data on the Web" [https://cacm.acm.org/magazines/2011/2/104398-structured-data-on-the-web/fulltext](https://cacm.acm.org/magazines/2011/2/104398-structured-data-on-the-web/fulltext). In this paper, Dr. Cafarella describes challenges and solutions for interacting with structured data on the web. The web is a repository of shared documents but it also contains a vast amount of structured data which covers all ranges of topics. This data shares similarities with data traditionally managed by database management systems with some additional unusual characteristics. For example, it is often injected in textual web pages and must be extracted before it is usable. There is no standardized data design like you might find in a conventional database. Similarly dislike traditional databases which focus on a single domain, or mini-world, the web covers literally every thing imaginable to humankind. Dr. Cafarella then describes two projects undertaken at Google Inc that are making progress towards making this data more accessible. The first project is Google WebTables. This project crawls the web seeking out small relational database tables which are expressed using HTML table tags. With additional data mining techniques WebTables is able to introduce new data-centric applications such as schema completion and synonym finding. The second project addressed in the paper is the Google Deep Web Crawler. This project attempts to discover information from the deep web, ie data on the web that is available only through filling out and submitting web forms, therefore it can not be crawled by traditional web crawlers. This project uses new web crawlers that submit relevant queries to a large number of web forms to extract data held in their deep webs. Dr. Crawler believes that these two projects are just the first steps toward exposing and managing structured Web data that is largely ignored by Web search engines.
